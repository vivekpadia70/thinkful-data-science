{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is about Natural Language Processing. Here, we will be using data from kaggle which contains comments from Reddit(https://www.kaggle.com/sherinclaudia/sarcastic-comments-on-reddit). Model needs to classify whether the comment is sarcastic or not. Data contains following features:\n",
    "    \n",
    "    Label:- sarcastic or not\n",
    "    comment:- Reply to a Parent Reddit comment\n",
    "    Author:- Person who commented\n",
    "    Subreddit:- Commented under which subreddit\n",
    "    Score:- Number of upvotes -(minus) Number of downvotes.\n",
    "    Ups:- Number of upvotes\n",
    "    Downs:- Number of downvotes\n",
    "    Date:- Commented date\n",
    "    Created_utc:- Commented time in the UTC Timezone\n",
    "    Parent_comment:- The Parent Reddit comment to which sarcastic replies are made\n",
    "\n",
    "Different models will be used to check which is best fit for this particular data. Deep learning will also be used for comparing LSTM model scores with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/vivek/Downloads/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data contains around 1.3 million rows so with limited computation power we will be deviding this dataset by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>505413.000000</td>\n",
       "      <td>505413.000000</td>\n",
       "      <td>505413.000000</td>\n",
       "      <td>505413.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.499924</td>\n",
       "      <td>6.966774</td>\n",
       "      <td>5.562488</td>\n",
       "      <td>-0.145616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.882056</td>\n",
       "      <td>43.002386</td>\n",
       "      <td>0.352721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-507.000000</td>\n",
       "      <td>-507.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9070.000000</td>\n",
       "      <td>5163.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label          score            ups          downs\n",
       "count  505413.000000  505413.000000  505413.000000  505413.000000\n",
       "mean        0.499924       6.966774       5.562488      -0.145616\n",
       "std         0.500000      50.882056      43.002386       0.352721\n",
       "min         0.000000    -507.000000    -507.000000      -1.000000\n",
       "25%         0.000000       1.000000       0.000000       0.000000\n",
       "50%         0.000000       2.000000       1.000000       0.000000\n",
       "75%         1.000000       4.000000       3.000000       0.000000\n",
       "max         1.000000    9070.000000    5163.000000       0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>608627</th>\n",
       "      <td>1</td>\n",
       "      <td>And he sure as hell is successful!</td>\n",
       "      <td>armiechedon</td>\n",
       "      <td>quityourbullshit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12</td>\n",
       "      <td>2015-12-09 23:41:48</td>\n",
       "      <td>Obama's smoked before? Or at least admitted it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456977</th>\n",
       "      <td>1</td>\n",
       "      <td>wait, you have to win the candidates to challe...</td>\n",
       "      <td>killingfeels</td>\n",
       "      <td>chess</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>2016-02-13 00:04:43</td>\n",
       "      <td>You can say the same thing about any chess pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803801</th>\n",
       "      <td>1</td>\n",
       "      <td>I mean how could anyone not see that it is ant...</td>\n",
       "      <td>Gnofar</td>\n",
       "      <td>katawashoujo</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12</td>\n",
       "      <td>2014-12-25 21:59:49</td>\n",
       "      <td>That isn't Hanako?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926708</th>\n",
       "      <td>0</td>\n",
       "      <td>Funny thing, most of the girl pants I've bough...</td>\n",
       "      <td>smischmal</td>\n",
       "      <td>transgender</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-03</td>\n",
       "      <td>2012-03-05 02:03:43</td>\n",
       "      <td>Girl Pants (X/post from TwoXChromosomes)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129279</th>\n",
       "      <td>0</td>\n",
       "      <td>Also Ranch dressing, because LBJ was Texan, an...</td>\n",
       "      <td>ElScreecho</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-19 19:20:53</td>\n",
       "      <td>Lyndon Johnson salad: A carrot and 2 beetroots...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                            comment  \\\n",
       "608627      1                 And he sure as hell is successful!   \n",
       "456977      1  wait, you have to win the candidates to challe...   \n",
       "803801      1  I mean how could anyone not see that it is ant...   \n",
       "926708      0  Funny thing, most of the girl pants I've bough...   \n",
       "129279      0  Also Ranch dressing, because LBJ was Texan, an...   \n",
       "\n",
       "              author         subreddit  score  ups  downs     date  \\\n",
       "608627   armiechedon  quityourbullshit      1    1      0  2015-12   \n",
       "456977  killingfeels             chess      1    1      0  2016-02   \n",
       "803801        Gnofar      katawashoujo      5    5      0  2014-12   \n",
       "926708     smischmal       transgender      2    2      0  2012-03   \n",
       "129279    ElScreecho         AskReddit     28   28      0  2016-09   \n",
       "\n",
       "                created_utc                                     parent_comment  \n",
       "608627  2015-12-09 23:41:48    Obama's smoked before? Or at least admitted it?  \n",
       "456977  2016-02-13 00:04:43  You can say the same thing about any chess pla...  \n",
       "803801  2014-12-25 21:59:49                                 That isn't Hanako?  \n",
       "926708  2012-03-05 02:03:43           Girl Pants (X/post from TwoXChromosomes)  \n",
       "129279  2016-09-19 19:20:53  Lyndon Johnson salad: A carrot and 2 beetroots...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 505413 entries, 608627 to 92160\n",
      "Data columns (total 10 columns):\n",
      "label             505413 non-null int64\n",
      "comment           505387 non-null object\n",
      "author            505413 non-null object\n",
      "subreddit         505413 non-null object\n",
      "score             505413 non-null int64\n",
      "ups               505413 non-null int64\n",
      "downs             505413 non-null int64\n",
      "date              505413 non-null object\n",
      "created_utc       505413 non-null object\n",
      "parent_comment    505413 non-null object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 42.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x12ac153d748>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(df[['score', 'ups', 'downs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303248, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303234, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Part Of Speech dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking Part Of Speech from every sentence and storing it into different dataframe pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw=0\n",
    "for sent in df['comment']:\n",
    "    tokens=nltk.word_tokenize(sent)\n",
    "    text = nltk.Text(tokens)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    for i in counts.most_common():\n",
    "        pos_df.loc[rw, i[0]] = i[1]\n",
    "    rw=rw+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns with punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CC', 'PRP', 'RB', 'IN', 'NN', 'VBZ', 'JJ', '.', 'TO', 'VB', 'DT', ',',\n",
       "       'VBP', 'NNS', 'WRB', 'MD', 'VBN', 'VBD', 'JJS', '(', 'PRP$', ')', 'JJR',\n",
       "       'VBG', 'NNP', 'RBR', ':', 'POS', 'CD', 'UH', 'WP', 'WDT', 'PDT', '``',\n",
       "       '''', 'NNPS', 'EX', 'RP', '$', '#', 'FW', 'RBS', 'WP$', 'SYM', 'LS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df=pos_df.drop(['.', ',', '``', \"''\", '(', ')', ':', '$', '#'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CC', 'PRP', 'RB', 'IN', 'NN', 'VBZ', 'JJ', 'TO', 'VB', 'DT', 'VBP',\n",
       "       'NNS', 'WRB', 'MD', 'VBN', 'VBD', 'JJS', 'PRP$', 'JJR', 'VBG', 'NNP',\n",
       "       'RBR', 'POS', 'CD', 'UH', 'WP', 'WDT', 'PDT', 'NNPS', 'EX', 'RP', 'FW',\n",
       "       'RBS', 'WP$', 'SYM', 'LS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3065cbc20dcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpos_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/vivek/Downloads/pos.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_df' is not defined"
     ]
    }
   ],
   "source": [
    "pos_df.to_csv('C:/Users/vivek/Downloads/pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.to_csv('C:/Users/vivek/Downloads/pos3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Part Of Speech dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_loaded_df = pd.read_csv('C:/Users/vivek/Downloads/pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_loaded_df = pd.read_csv('C:/Users/vivek/Downloads/pos3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_loaded_df = pos_loaded_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PCA on part of speech dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.51616500e+05 -7.61128641e-01 -6.30917925e-01 ... -2.83761445e-01\n",
      "  -1.46034865e-01 -8.53093605e-01]\n",
      " [ 1.51615500e+05 -7.54616647e-01  1.39582071e+00 ...  4.03112692e-01\n",
      "   5.61308488e-01  2.08408713e+00]\n",
      " [ 1.51614500e+05 -7.61457838e-01  3.73037101e-01 ... -4.67805906e-01\n",
      "   3.40106825e-01 -2.37060380e-01]\n",
      " ...\n",
      " [-1.51614500e+05  2.13975086e-01 -1.72083725e+00 ...  6.02611203e-01\n",
      "  -2.64467943e-01  3.47645683e-01]\n",
      " [-1.51615500e+05  2.09673211e-01 -1.73476540e+00 ... -7.66005745e-01\n",
      "   8.56682314e-01 -5.84257624e-01]\n",
      " [-1.51616500e+05  1.21511792e+00 -1.54435909e+00 ...  7.35531952e-02\n",
      "  -1.11500492e+00 -6.68351144e-01]]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(10)\n",
    "pos_pca = pca.fit_transform(pos_loaded_df)\n",
    "print(pos_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pca_df = pd.DataFrame(pos_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pca_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>608627</th>\n",
       "      <td>151616.500000</td>\n",
       "      <td>-0.761129</td>\n",
       "      <td>-0.630918</td>\n",
       "      <td>0.222937</td>\n",
       "      <td>0.353453</td>\n",
       "      <td>0.235507</td>\n",
       "      <td>0.409962</td>\n",
       "      <td>-0.283761</td>\n",
       "      <td>-0.146035</td>\n",
       "      <td>-0.853094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456977</th>\n",
       "      <td>151615.500000</td>\n",
       "      <td>-0.754617</td>\n",
       "      <td>1.395821</td>\n",
       "      <td>-0.642808</td>\n",
       "      <td>-0.163310</td>\n",
       "      <td>0.093172</td>\n",
       "      <td>-0.907329</td>\n",
       "      <td>0.403113</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>2.084087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803801</th>\n",
       "      <td>151614.500000</td>\n",
       "      <td>-0.761458</td>\n",
       "      <td>0.373037</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.178419</td>\n",
       "      <td>1.454610</td>\n",
       "      <td>-0.034679</td>\n",
       "      <td>-0.467806</td>\n",
       "      <td>0.340107</td>\n",
       "      <td>-0.237060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926708</th>\n",
       "      <td>151613.500000</td>\n",
       "      <td>-0.745755</td>\n",
       "      <td>3.584052</td>\n",
       "      <td>-5.154209</td>\n",
       "      <td>-10.490453</td>\n",
       "      <td>0.197803</td>\n",
       "      <td>3.029354</td>\n",
       "      <td>-4.868224</td>\n",
       "      <td>-0.784526</td>\n",
       "      <td>-0.434862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129279</th>\n",
       "      <td>151612.500000</td>\n",
       "      <td>2.239032</td>\n",
       "      <td>0.402433</td>\n",
       "      <td>-0.554391</td>\n",
       "      <td>-0.533961</td>\n",
       "      <td>1.170023</td>\n",
       "      <td>1.715513</td>\n",
       "      <td>2.285805</td>\n",
       "      <td>-1.691601</td>\n",
       "      <td>0.164498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951335</th>\n",
       "      <td>151611.500000</td>\n",
       "      <td>-0.768255</td>\n",
       "      <td>-1.725251</td>\n",
       "      <td>0.965991</td>\n",
       "      <td>1.450976</td>\n",
       "      <td>-0.356155</td>\n",
       "      <td>0.370146</td>\n",
       "      <td>-0.447141</td>\n",
       "      <td>0.256995</td>\n",
       "      <td>-0.097909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655372</th>\n",
       "      <td>151610.500000</td>\n",
       "      <td>-0.759314</td>\n",
       "      <td>-0.618679</td>\n",
       "      <td>0.230855</td>\n",
       "      <td>0.381738</td>\n",
       "      <td>-1.254519</td>\n",
       "      <td>0.040779</td>\n",
       "      <td>-1.644416</td>\n",
       "      <td>-1.196141</td>\n",
       "      <td>0.427067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632636</th>\n",
       "      <td>151609.500000</td>\n",
       "      <td>4.246648</td>\n",
       "      <td>1.405318</td>\n",
       "      <td>-1.053863</td>\n",
       "      <td>-0.671149</td>\n",
       "      <td>0.104611</td>\n",
       "      <td>-0.757208</td>\n",
       "      <td>1.324996</td>\n",
       "      <td>-0.197571</td>\n",
       "      <td>1.464755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397220</th>\n",
       "      <td>151608.500000</td>\n",
       "      <td>-0.768397</td>\n",
       "      <td>-1.544883</td>\n",
       "      <td>0.318015</td>\n",
       "      <td>-0.141907</td>\n",
       "      <td>0.035002</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>-0.140494</td>\n",
       "      <td>0.182244</td>\n",
       "      <td>-0.678119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412260</th>\n",
       "      <td>151607.500000</td>\n",
       "      <td>-0.765090</td>\n",
       "      <td>-0.558702</td>\n",
       "      <td>-0.053799</td>\n",
       "      <td>-0.344552</td>\n",
       "      <td>0.624242</td>\n",
       "      <td>-0.158696</td>\n",
       "      <td>0.635071</td>\n",
       "      <td>-0.405574</td>\n",
       "      <td>1.765062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913023</th>\n",
       "      <td>151606.500000</td>\n",
       "      <td>-0.772793</td>\n",
       "      <td>-1.669649</td>\n",
       "      <td>0.659140</td>\n",
       "      <td>0.543560</td>\n",
       "      <td>1.719706</td>\n",
       "      <td>-0.789494</td>\n",
       "      <td>0.115744</td>\n",
       "      <td>0.608837</td>\n",
       "      <td>1.226919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193954</th>\n",
       "      <td>151605.500000</td>\n",
       "      <td>-0.765158</td>\n",
       "      <td>-0.745903</td>\n",
       "      <td>0.606154</td>\n",
       "      <td>1.269889</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>-0.576983</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>0.225435</td>\n",
       "      <td>0.359741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770346</th>\n",
       "      <td>151604.500000</td>\n",
       "      <td>-0.762011</td>\n",
       "      <td>-0.642187</td>\n",
       "      <td>0.259326</td>\n",
       "      <td>0.373597</td>\n",
       "      <td>-0.172963</td>\n",
       "      <td>0.487578</td>\n",
       "      <td>-1.138748</td>\n",
       "      <td>-1.488620</td>\n",
       "      <td>0.435725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249317</th>\n",
       "      <td>151603.500000</td>\n",
       "      <td>-0.761822</td>\n",
       "      <td>-0.731718</td>\n",
       "      <td>0.562843</td>\n",
       "      <td>1.156863</td>\n",
       "      <td>0.227859</td>\n",
       "      <td>-0.879937</td>\n",
       "      <td>-0.383273</td>\n",
       "      <td>-0.385442</td>\n",
       "      <td>-0.262685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831748</th>\n",
       "      <td>151602.500000</td>\n",
       "      <td>1.237262</td>\n",
       "      <td>-0.603106</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>0.118185</td>\n",
       "      <td>-0.577832</td>\n",
       "      <td>0.566963</td>\n",
       "      <td>0.149398</td>\n",
       "      <td>-0.380130</td>\n",
       "      <td>0.424371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20132</th>\n",
       "      <td>151601.500000</td>\n",
       "      <td>-0.713410</td>\n",
       "      <td>2.836417</td>\n",
       "      <td>-0.698091</td>\n",
       "      <td>-2.056818</td>\n",
       "      <td>0.612333</td>\n",
       "      <td>-0.470974</td>\n",
       "      <td>-0.171978</td>\n",
       "      <td>1.310899</td>\n",
       "      <td>-0.597386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161090</th>\n",
       "      <td>151600.500000</td>\n",
       "      <td>1.231263</td>\n",
       "      <td>-1.743601</td>\n",
       "      <td>0.883458</td>\n",
       "      <td>1.468917</td>\n",
       "      <td>-0.551553</td>\n",
       "      <td>-0.322844</td>\n",
       "      <td>0.498577</td>\n",
       "      <td>0.213889</td>\n",
       "      <td>0.086036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223589</th>\n",
       "      <td>151599.500000</td>\n",
       "      <td>0.251957</td>\n",
       "      <td>1.372716</td>\n",
       "      <td>-0.597216</td>\n",
       "      <td>0.045065</td>\n",
       "      <td>-0.448647</td>\n",
       "      <td>-0.589845</td>\n",
       "      <td>-0.463292</td>\n",
       "      <td>-1.005811</td>\n",
       "      <td>1.531076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274120</th>\n",
       "      <td>151598.500000</td>\n",
       "      <td>-0.770480</td>\n",
       "      <td>-1.247992</td>\n",
       "      <td>-0.755466</td>\n",
       "      <td>-2.812329</td>\n",
       "      <td>0.624099</td>\n",
       "      <td>1.881672</td>\n",
       "      <td>1.765628</td>\n",
       "      <td>-1.418888</td>\n",
       "      <td>-2.108018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537152</th>\n",
       "      <td>151597.500000</td>\n",
       "      <td>2.273106</td>\n",
       "      <td>-1.318857</td>\n",
       "      <td>1.281615</td>\n",
       "      <td>0.077540</td>\n",
       "      <td>-1.126230</td>\n",
       "      <td>-1.182062</td>\n",
       "      <td>0.218807</td>\n",
       "      <td>-0.771301</td>\n",
       "      <td>-0.326992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255424</th>\n",
       "      <td>151596.500000</td>\n",
       "      <td>0.231687</td>\n",
       "      <td>-1.789585</td>\n",
       "      <td>1.091684</td>\n",
       "      <td>1.805935</td>\n",
       "      <td>-0.094399</td>\n",
       "      <td>-0.155950</td>\n",
       "      <td>-0.028961</td>\n",
       "      <td>-0.160557</td>\n",
       "      <td>0.055701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407177</th>\n",
       "      <td>151595.500000</td>\n",
       "      <td>0.257929</td>\n",
       "      <td>-0.245059</td>\n",
       "      <td>0.469980</td>\n",
       "      <td>-1.791547</td>\n",
       "      <td>3.660854</td>\n",
       "      <td>-0.464372</td>\n",
       "      <td>-0.747088</td>\n",
       "      <td>0.469398</td>\n",
       "      <td>2.631799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635566</th>\n",
       "      <td>151594.500000</td>\n",
       "      <td>1.240688</td>\n",
       "      <td>-0.629810</td>\n",
       "      <td>0.176580</td>\n",
       "      <td>0.648646</td>\n",
       "      <td>-1.313930</td>\n",
       "      <td>-0.890063</td>\n",
       "      <td>1.022706</td>\n",
       "      <td>0.219142</td>\n",
       "      <td>-0.435493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175276</th>\n",
       "      <td>151593.500000</td>\n",
       "      <td>-0.762925</td>\n",
       "      <td>-0.677710</td>\n",
       "      <td>0.403730</td>\n",
       "      <td>0.767311</td>\n",
       "      <td>-0.497069</td>\n",
       "      <td>-0.164724</td>\n",
       "      <td>-1.548441</td>\n",
       "      <td>-1.176487</td>\n",
       "      <td>0.684267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972149</th>\n",
       "      <td>151592.500000</td>\n",
       "      <td>0.227774</td>\n",
       "      <td>-1.442876</td>\n",
       "      <td>-0.122363</td>\n",
       "      <td>-1.137036</td>\n",
       "      <td>-0.277445</td>\n",
       "      <td>1.896786</td>\n",
       "      <td>-0.936051</td>\n",
       "      <td>0.479493</td>\n",
       "      <td>-0.863009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902815</th>\n",
       "      <td>151591.500000</td>\n",
       "      <td>7.226628</td>\n",
       "      <td>-1.891970</td>\n",
       "      <td>0.866898</td>\n",
       "      <td>1.871903</td>\n",
       "      <td>-0.073115</td>\n",
       "      <td>-0.151829</td>\n",
       "      <td>-0.033626</td>\n",
       "      <td>-0.145781</td>\n",
       "      <td>0.061133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159302</th>\n",
       "      <td>151590.499999</td>\n",
       "      <td>6.261066</td>\n",
       "      <td>4.371595</td>\n",
       "      <td>-2.245032</td>\n",
       "      <td>-1.029118</td>\n",
       "      <td>-0.532939</td>\n",
       "      <td>0.335746</td>\n",
       "      <td>1.415288</td>\n",
       "      <td>0.401382</td>\n",
       "      <td>-0.651711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884232</th>\n",
       "      <td>151589.500000</td>\n",
       "      <td>-0.764378</td>\n",
       "      <td>-0.594592</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>-0.085799</td>\n",
       "      <td>0.787437</td>\n",
       "      <td>-0.499170</td>\n",
       "      <td>0.106160</td>\n",
       "      <td>-0.092536</td>\n",
       "      <td>0.110561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590961</th>\n",
       "      <td>151588.499999</td>\n",
       "      <td>1.323475</td>\n",
       "      <td>2.689883</td>\n",
       "      <td>1.581539</td>\n",
       "      <td>1.144264</td>\n",
       "      <td>-0.562398</td>\n",
       "      <td>0.016904</td>\n",
       "      <td>-0.652478</td>\n",
       "      <td>-0.267654</td>\n",
       "      <td>-0.192259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419507</th>\n",
       "      <td>151587.500000</td>\n",
       "      <td>1.276677</td>\n",
       "      <td>0.625502</td>\n",
       "      <td>0.659385</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.294975</td>\n",
       "      <td>1.186190</td>\n",
       "      <td>-0.580320</td>\n",
       "      <td>-1.240990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461807</th>\n",
       "      <td>-151587.500000</td>\n",
       "      <td>-0.785799</td>\n",
       "      <td>-0.787622</td>\n",
       "      <td>0.576151</td>\n",
       "      <td>1.181732</td>\n",
       "      <td>0.448433</td>\n",
       "      <td>0.670143</td>\n",
       "      <td>0.193351</td>\n",
       "      <td>-0.735131</td>\n",
       "      <td>1.071242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91981</th>\n",
       "      <td>-151588.500000</td>\n",
       "      <td>-0.782933</td>\n",
       "      <td>-0.862242</td>\n",
       "      <td>0.877456</td>\n",
       "      <td>1.997242</td>\n",
       "      <td>-0.070231</td>\n",
       "      <td>-0.146314</td>\n",
       "      <td>-0.044265</td>\n",
       "      <td>-0.177490</td>\n",
       "      <td>0.058737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136611</th>\n",
       "      <td>-151589.500000</td>\n",
       "      <td>-0.766660</td>\n",
       "      <td>1.956188</td>\n",
       "      <td>-2.790299</td>\n",
       "      <td>-5.496217</td>\n",
       "      <td>0.989016</td>\n",
       "      <td>-0.249447</td>\n",
       "      <td>-0.664985</td>\n",
       "      <td>-2.662989</td>\n",
       "      <td>-1.830882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106304</th>\n",
       "      <td>-151590.500000</td>\n",
       "      <td>-0.790238</td>\n",
       "      <td>-1.744246</td>\n",
       "      <td>0.805144</td>\n",
       "      <td>0.983304</td>\n",
       "      <td>1.150948</td>\n",
       "      <td>-0.992739</td>\n",
       "      <td>-0.283217</td>\n",
       "      <td>0.458423</td>\n",
       "      <td>-0.576949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117505</th>\n",
       "      <td>-151591.500000</td>\n",
       "      <td>-0.784105</td>\n",
       "      <td>0.302882</td>\n",
       "      <td>-0.152170</td>\n",
       "      <td>0.177196</td>\n",
       "      <td>0.587192</td>\n",
       "      <td>1.184224</td>\n",
       "      <td>-0.024064</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>-0.485009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9929</th>\n",
       "      <td>-151592.500000</td>\n",
       "      <td>-0.771148</td>\n",
       "      <td>1.725285</td>\n",
       "      <td>-1.927398</td>\n",
       "      <td>-3.209993</td>\n",
       "      <td>-0.035532</td>\n",
       "      <td>-0.780492</td>\n",
       "      <td>-1.209977</td>\n",
       "      <td>0.886804</td>\n",
       "      <td>-1.003386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632255</th>\n",
       "      <td>-151593.500000</td>\n",
       "      <td>-0.782064</td>\n",
       "      <td>0.206495</td>\n",
       "      <td>0.206344</td>\n",
       "      <td>1.052590</td>\n",
       "      <td>0.198480</td>\n",
       "      <td>1.751806</td>\n",
       "      <td>-0.114561</td>\n",
       "      <td>-0.829654</td>\n",
       "      <td>-0.264377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33405</th>\n",
       "      <td>-151594.500000</td>\n",
       "      <td>-0.703878</td>\n",
       "      <td>0.899470</td>\n",
       "      <td>1.601197</td>\n",
       "      <td>-0.544303</td>\n",
       "      <td>0.496270</td>\n",
       "      <td>-0.539603</td>\n",
       "      <td>0.243920</td>\n",
       "      <td>-1.086151</td>\n",
       "      <td>0.054009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776609</th>\n",
       "      <td>-151595.500000</td>\n",
       "      <td>3.239240</td>\n",
       "      <td>3.473755</td>\n",
       "      <td>-2.114798</td>\n",
       "      <td>-1.754274</td>\n",
       "      <td>-0.491084</td>\n",
       "      <td>-0.152494</td>\n",
       "      <td>0.423551</td>\n",
       "      <td>-0.586852</td>\n",
       "      <td>-1.769550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713866</th>\n",
       "      <td>-151596.500000</td>\n",
       "      <td>1.211741</td>\n",
       "      <td>-1.741780</td>\n",
       "      <td>0.699565</td>\n",
       "      <td>1.007343</td>\n",
       "      <td>-0.092237</td>\n",
       "      <td>1.141916</td>\n",
       "      <td>0.067451</td>\n",
       "      <td>0.073421</td>\n",
       "      <td>-0.532474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463475</th>\n",
       "      <td>-151597.500000</td>\n",
       "      <td>-0.792210</td>\n",
       "      <td>-0.636680</td>\n",
       "      <td>-0.037479</td>\n",
       "      <td>-0.433705</td>\n",
       "      <td>1.762678</td>\n",
       "      <td>0.792886</td>\n",
       "      <td>-0.555148</td>\n",
       "      <td>0.263043</td>\n",
       "      <td>2.225687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386062</th>\n",
       "      <td>-151598.500000</td>\n",
       "      <td>-0.782933</td>\n",
       "      <td>-0.862243</td>\n",
       "      <td>0.877456</td>\n",
       "      <td>1.997242</td>\n",
       "      <td>-0.070232</td>\n",
       "      <td>-0.146314</td>\n",
       "      <td>-0.044265</td>\n",
       "      <td>-0.177491</td>\n",
       "      <td>0.058737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347723</th>\n",
       "      <td>-151599.500000</td>\n",
       "      <td>-0.773665</td>\n",
       "      <td>1.038912</td>\n",
       "      <td>0.429060</td>\n",
       "      <td>2.416958</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-0.136247</td>\n",
       "      <td>-0.071050</td>\n",
       "      <td>-0.196600</td>\n",
       "      <td>0.062937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194778</th>\n",
       "      <td>-151600.500000</td>\n",
       "      <td>-0.784332</td>\n",
       "      <td>-0.649339</td>\n",
       "      <td>0.080283</td>\n",
       "      <td>-0.009882</td>\n",
       "      <td>1.243054</td>\n",
       "      <td>-0.418406</td>\n",
       "      <td>0.238451</td>\n",
       "      <td>-0.218528</td>\n",
       "      <td>-0.499252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609579</th>\n",
       "      <td>-151601.500000</td>\n",
       "      <td>-0.778631</td>\n",
       "      <td>0.567986</td>\n",
       "      <td>-1.059528</td>\n",
       "      <td>-2.002742</td>\n",
       "      <td>1.256707</td>\n",
       "      <td>0.088463</td>\n",
       "      <td>-0.830138</td>\n",
       "      <td>0.624999</td>\n",
       "      <td>-2.047526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911738</th>\n",
       "      <td>-151602.500000</td>\n",
       "      <td>0.223475</td>\n",
       "      <td>0.200134</td>\n",
       "      <td>0.194488</td>\n",
       "      <td>1.194594</td>\n",
       "      <td>-0.107541</td>\n",
       "      <td>0.140398</td>\n",
       "      <td>0.252402</td>\n",
       "      <td>-1.041241</td>\n",
       "      <td>-0.539428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934770</th>\n",
       "      <td>-151603.500000</td>\n",
       "      <td>-0.711446</td>\n",
       "      <td>-0.925958</td>\n",
       "      <td>1.872879</td>\n",
       "      <td>-1.299852</td>\n",
       "      <td>-1.476246</td>\n",
       "      <td>0.529734</td>\n",
       "      <td>-0.721256</td>\n",
       "      <td>-1.153303</td>\n",
       "      <td>0.241363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306800</th>\n",
       "      <td>-151604.500000</td>\n",
       "      <td>-0.782762</td>\n",
       "      <td>-0.679379</td>\n",
       "      <td>0.238002</td>\n",
       "      <td>0.433216</td>\n",
       "      <td>-0.204320</td>\n",
       "      <td>-0.889254</td>\n",
       "      <td>-0.605933</td>\n",
       "      <td>-0.127533</td>\n",
       "      <td>0.080676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528245</th>\n",
       "      <td>-151605.500000</td>\n",
       "      <td>1.221815</td>\n",
       "      <td>0.571166</td>\n",
       "      <td>-1.110884</td>\n",
       "      <td>-1.677211</td>\n",
       "      <td>-0.507288</td>\n",
       "      <td>-1.107916</td>\n",
       "      <td>1.008598</td>\n",
       "      <td>0.992560</td>\n",
       "      <td>2.114727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895092</th>\n",
       "      <td>-151606.500000</td>\n",
       "      <td>2.227219</td>\n",
       "      <td>1.420789</td>\n",
       "      <td>-1.106141</td>\n",
       "      <td>-0.978890</td>\n",
       "      <td>-0.234202</td>\n",
       "      <td>-1.311495</td>\n",
       "      <td>-1.130708</td>\n",
       "      <td>0.729755</td>\n",
       "      <td>-0.537971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772584</th>\n",
       "      <td>-151607.500000</td>\n",
       "      <td>-0.770044</td>\n",
       "      <td>1.283467</td>\n",
       "      <td>-0.306681</td>\n",
       "      <td>0.862796</td>\n",
       "      <td>-1.553218</td>\n",
       "      <td>-0.216520</td>\n",
       "      <td>0.550005</td>\n",
       "      <td>0.635150</td>\n",
       "      <td>-0.357268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618410</th>\n",
       "      <td>-151608.500000</td>\n",
       "      <td>-0.791118</td>\n",
       "      <td>-1.352236</td>\n",
       "      <td>-0.437018</td>\n",
       "      <td>-1.888479</td>\n",
       "      <td>-2.388688</td>\n",
       "      <td>1.538555</td>\n",
       "      <td>-1.784206</td>\n",
       "      <td>0.050809</td>\n",
       "      <td>1.195001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703155</th>\n",
       "      <td>-151609.500000</td>\n",
       "      <td>-0.784336</td>\n",
       "      <td>-1.697148</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>0.909698</td>\n",
       "      <td>-0.568205</td>\n",
       "      <td>-0.242968</td>\n",
       "      <td>-0.738272</td>\n",
       "      <td>-0.901610</td>\n",
       "      <td>0.401864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506761</th>\n",
       "      <td>-151610.500000</td>\n",
       "      <td>0.222422</td>\n",
       "      <td>0.298844</td>\n",
       "      <td>-0.122274</td>\n",
       "      <td>0.460425</td>\n",
       "      <td>-0.239948</td>\n",
       "      <td>0.057379</td>\n",
       "      <td>0.572905</td>\n",
       "      <td>-0.781645</td>\n",
       "      <td>0.855798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336440</th>\n",
       "      <td>-151611.500000</td>\n",
       "      <td>-0.782884</td>\n",
       "      <td>-0.799613</td>\n",
       "      <td>0.693169</td>\n",
       "      <td>1.632717</td>\n",
       "      <td>-0.532910</td>\n",
       "      <td>-0.322543</td>\n",
       "      <td>0.483360</td>\n",
       "      <td>0.203929</td>\n",
       "      <td>0.089317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308261</th>\n",
       "      <td>-151612.500000</td>\n",
       "      <td>0.211154</td>\n",
       "      <td>-1.790529</td>\n",
       "      <td>0.954432</td>\n",
       "      <td>1.546197</td>\n",
       "      <td>-0.340676</td>\n",
       "      <td>0.392113</td>\n",
       "      <td>-0.411337</td>\n",
       "      <td>0.305169</td>\n",
       "      <td>-0.080888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129263</th>\n",
       "      <td>-151613.500000</td>\n",
       "      <td>-0.779831</td>\n",
       "      <td>0.329673</td>\n",
       "      <td>-0.117612</td>\n",
       "      <td>0.531989</td>\n",
       "      <td>-0.732079</td>\n",
       "      <td>-0.018468</td>\n",
       "      <td>2.194183</td>\n",
       "      <td>0.608724</td>\n",
       "      <td>0.808014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361111</th>\n",
       "      <td>-151614.500000</td>\n",
       "      <td>0.213975</td>\n",
       "      <td>-1.720837</td>\n",
       "      <td>0.697943</td>\n",
       "      <td>0.888751</td>\n",
       "      <td>0.416141</td>\n",
       "      <td>0.725298</td>\n",
       "      <td>0.602611</td>\n",
       "      <td>-0.264468</td>\n",
       "      <td>0.347646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369605</th>\n",
       "      <td>-151615.500000</td>\n",
       "      <td>0.209673</td>\n",
       "      <td>-1.734765</td>\n",
       "      <td>0.714444</td>\n",
       "      <td>0.905580</td>\n",
       "      <td>0.371579</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>-0.766006</td>\n",
       "      <td>0.856682</td>\n",
       "      <td>-0.584258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809346</th>\n",
       "      <td>-151616.500000</td>\n",
       "      <td>1.215118</td>\n",
       "      <td>-1.544359</td>\n",
       "      <td>0.053696</td>\n",
       "      <td>-0.543328</td>\n",
       "      <td>-0.308650</td>\n",
       "      <td>-0.866583</td>\n",
       "      <td>0.073553</td>\n",
       "      <td>-1.115005</td>\n",
       "      <td>-0.668351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303234 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3          4         5  \\\n",
       "608627  151616.500000 -0.761129 -0.630918  0.222937   0.353453  0.235507   \n",
       "456977  151615.500000 -0.754617  1.395821 -0.642808  -0.163310  0.093172   \n",
       "803801  151614.500000 -0.761458  0.373037 -0.265671  -0.178419  1.454610   \n",
       "926708  151613.500000 -0.745755  3.584052 -5.154209 -10.490453  0.197803   \n",
       "129279  151612.500000  2.239032  0.402433 -0.554391  -0.533961  1.170023   \n",
       "951335  151611.500000 -0.768255 -1.725251  0.965991   1.450976 -0.356155   \n",
       "655372  151610.500000 -0.759314 -0.618679  0.230855   0.381738 -1.254519   \n",
       "632636  151609.500000  4.246648  1.405318 -1.053863  -0.671149  0.104611   \n",
       "397220  151608.500000 -0.768397 -1.544883  0.318015  -0.141907  0.035002   \n",
       "412260  151607.500000 -0.765090 -0.558702 -0.053799  -0.344552  0.624242   \n",
       "913023  151606.500000 -0.772793 -1.669649  0.659140   0.543560  1.719706   \n",
       "193954  151605.500000 -0.765158 -0.745903  0.606154   1.269889  0.898305   \n",
       "770346  151604.500000 -0.762011 -0.642187  0.259326   0.373597 -0.172963   \n",
       "249317  151603.500000 -0.761822 -0.731718  0.562843   1.156863  0.227859   \n",
       "831748  151602.500000  1.237262 -0.603106  0.018118   0.118185 -0.577832   \n",
       "20132   151601.500000 -0.713410  2.836417 -0.698091  -2.056818  0.612333   \n",
       "161090  151600.500000  1.231263 -1.743601  0.883458   1.468917 -0.551553   \n",
       "223589  151599.500000  0.251957  1.372716 -0.597216   0.045065 -0.448647   \n",
       "274120  151598.500000 -0.770480 -1.247992 -0.755466  -2.812329  0.624099   \n",
       "537152  151597.500000  2.273106 -1.318857  1.281615   0.077540 -1.126230   \n",
       "255424  151596.500000  0.231687 -1.789585  1.091684   1.805935 -0.094399   \n",
       "407177  151595.500000  0.257929 -0.245059  0.469980  -1.791547  3.660854   \n",
       "635566  151594.500000  1.240688 -0.629810  0.176580   0.648646 -1.313930   \n",
       "175276  151593.500000 -0.762925 -0.677710  0.403730   0.767311 -0.497069   \n",
       "972149  151592.500000  0.227774 -1.442876 -0.122363  -1.137036 -0.277445   \n",
       "902815  151591.500000  7.226628 -1.891970  0.866898   1.871903 -0.073115   \n",
       "159302  151590.499999  6.261066  4.371595 -2.245032  -1.029118 -0.532939   \n",
       "884232  151589.500000 -0.764378 -0.594592  0.055654  -0.085799  0.787437   \n",
       "590961  151588.499999  1.323475  2.689883  1.581539   1.144264 -0.562398   \n",
       "419507  151587.500000  1.276677  0.625502  0.659385  -0.136859  0.263200   \n",
       "...               ...       ...       ...       ...        ...       ...   \n",
       "461807 -151587.500000 -0.785799 -0.787622  0.576151   1.181732  0.448433   \n",
       "91981  -151588.500000 -0.782933 -0.862242  0.877456   1.997242 -0.070231   \n",
       "136611 -151589.500000 -0.766660  1.956188 -2.790299  -5.496217  0.989016   \n",
       "106304 -151590.500000 -0.790238 -1.744246  0.805144   0.983304  1.150948   \n",
       "117505 -151591.500000 -0.784105  0.302882 -0.152170   0.177196  0.587192   \n",
       "9929   -151592.500000 -0.771148  1.725285 -1.927398  -3.209993 -0.035532   \n",
       "632255 -151593.500000 -0.782064  0.206495  0.206344   1.052590  0.198480   \n",
       "33405  -151594.500000 -0.703878  0.899470  1.601197  -0.544303  0.496270   \n",
       "776609 -151595.500000  3.239240  3.473755 -2.114798  -1.754274 -0.491084   \n",
       "713866 -151596.500000  1.211741 -1.741780  0.699565   1.007343 -0.092237   \n",
       "463475 -151597.500000 -0.792210 -0.636680 -0.037479  -0.433705  1.762678   \n",
       "386062 -151598.500000 -0.782933 -0.862243  0.877456   1.997242 -0.070232   \n",
       "347723 -151599.500000 -0.773665  1.038912  0.429060   2.416958  0.002479   \n",
       "194778 -151600.500000 -0.784332 -0.649339  0.080283  -0.009882  1.243054   \n",
       "609579 -151601.500000 -0.778631  0.567986 -1.059528  -2.002742  1.256707   \n",
       "911738 -151602.500000  0.223475  0.200134  0.194488   1.194594 -0.107541   \n",
       "934770 -151603.500000 -0.711446 -0.925958  1.872879  -1.299852 -1.476246   \n",
       "306800 -151604.500000 -0.782762 -0.679379  0.238002   0.433216 -0.204320   \n",
       "528245 -151605.500000  1.221815  0.571166 -1.110884  -1.677211 -0.507288   \n",
       "895092 -151606.500000  2.227219  1.420789 -1.106141  -0.978890 -0.234202   \n",
       "772584 -151607.500000 -0.770044  1.283467 -0.306681   0.862796 -1.553218   \n",
       "618410 -151608.500000 -0.791118 -1.352236 -0.437018  -1.888479 -2.388688   \n",
       "703155 -151609.500000 -0.784336 -1.697148  0.735592   0.909698 -0.568205   \n",
       "506761 -151610.500000  0.222422  0.298844 -0.122274   0.460425 -0.239948   \n",
       "336440 -151611.500000 -0.782884 -0.799613  0.693169   1.632717 -0.532910   \n",
       "308261 -151612.500000  0.211154 -1.790529  0.954432   1.546197 -0.340676   \n",
       "129263 -151613.500000 -0.779831  0.329673 -0.117612   0.531989 -0.732079   \n",
       "361111 -151614.500000  0.213975 -1.720837  0.697943   0.888751  0.416141   \n",
       "369605 -151615.500000  0.209673 -1.734765  0.714444   0.905580  0.371579   \n",
       "809346 -151616.500000  1.215118 -1.544359  0.053696  -0.543328 -0.308650   \n",
       "\n",
       "               6         7         8         9  \n",
       "608627  0.409962 -0.283761 -0.146035 -0.853094  \n",
       "456977 -0.907329  0.403113  0.561308  2.084087  \n",
       "803801 -0.034679 -0.467806  0.340107 -0.237060  \n",
       "926708  3.029354 -4.868224 -0.784526 -0.434862  \n",
       "129279  1.715513  2.285805 -1.691601  0.164498  \n",
       "951335  0.370146 -0.447141  0.256995 -0.097909  \n",
       "655372  0.040779 -1.644416 -1.196141  0.427067  \n",
       "632636 -0.757208  1.324996 -0.197571  1.464755  \n",
       "397220  0.031408 -0.140494  0.182244 -0.678119  \n",
       "412260 -0.158696  0.635071 -0.405574  1.765062  \n",
       "913023 -0.789494  0.115744  0.608837  1.226919  \n",
       "193954 -0.576983 -0.003905  0.225435  0.359741  \n",
       "770346  0.487578 -1.138748 -1.488620  0.435725  \n",
       "249317 -0.879937 -0.383273 -0.385442 -0.262685  \n",
       "831748  0.566963  0.149398 -0.380130  0.424371  \n",
       "20132  -0.470974 -0.171978  1.310899 -0.597386  \n",
       "161090 -0.322844  0.498577  0.213889  0.086036  \n",
       "223589 -0.589845 -0.463292 -1.005811  1.531076  \n",
       "274120  1.881672  1.765628 -1.418888 -2.108018  \n",
       "537152 -1.182062  0.218807 -0.771301 -0.326992  \n",
       "255424 -0.155950 -0.028961 -0.160557  0.055701  \n",
       "407177 -0.464372 -0.747088  0.469398  2.631799  \n",
       "635566 -0.890063  1.022706  0.219142 -0.435493  \n",
       "175276 -0.164724 -1.548441 -1.176487  0.684267  \n",
       "972149  1.896786 -0.936051  0.479493 -0.863009  \n",
       "902815 -0.151829 -0.033626 -0.145781  0.061133  \n",
       "159302  0.335746  1.415288  0.401382 -0.651711  \n",
       "884232 -0.499170  0.106160 -0.092536  0.110561  \n",
       "590961  0.016904 -0.652478 -0.267654 -0.192259  \n",
       "419507  0.294975  1.186190 -0.580320 -1.240990  \n",
       "...          ...       ...       ...       ...  \n",
       "461807  0.670143  0.193351 -0.735131  1.071242  \n",
       "91981  -0.146314 -0.044265 -0.177490  0.058737  \n",
       "136611 -0.249447 -0.664985 -2.662989 -1.830882  \n",
       "106304 -0.992739 -0.283217  0.458423 -0.576949  \n",
       "117505  1.184224 -0.024064  0.009532 -0.485009  \n",
       "9929   -0.780492 -1.209977  0.886804 -1.003386  \n",
       "632255  1.751806 -0.114561 -0.829654 -0.264377  \n",
       "33405  -0.539603  0.243920 -1.086151  0.054009  \n",
       "776609 -0.152494  0.423551 -0.586852 -1.769550  \n",
       "713866  1.141916  0.067451  0.073421 -0.532474  \n",
       "463475  0.792886 -0.555148  0.263043  2.225687  \n",
       "386062 -0.146314 -0.044265 -0.177491  0.058737  \n",
       "347723 -0.136247 -0.071050 -0.196600  0.062937  \n",
       "194778 -0.418406  0.238451 -0.218528 -0.499252  \n",
       "609579  0.088463 -0.830138  0.624999 -2.047526  \n",
       "911738  0.140398  0.252402 -1.041241 -0.539428  \n",
       "934770  0.529734 -0.721256 -1.153303  0.241363  \n",
       "306800 -0.889254 -0.605933 -0.127533  0.080676  \n",
       "528245 -1.107916  1.008598  0.992560  2.114727  \n",
       "895092 -1.311495 -1.130708  0.729755 -0.537971  \n",
       "772584 -0.216520  0.550005  0.635150 -0.357268  \n",
       "618410  1.538555 -1.784206  0.050809  1.195001  \n",
       "703155 -0.242968 -0.738272 -0.901610  0.401864  \n",
       "506761  0.057379  0.572905 -0.781645  0.855798  \n",
       "336440 -0.322543  0.483360  0.203929  0.089317  \n",
       "308261  0.392113 -0.411337  0.305169 -0.080888  \n",
       "129263 -0.018468  2.194183  0.608724  0.808014  \n",
       "361111  0.725298  0.602611 -0.264468  0.347646  \n",
       "369605  0.076218 -0.766006  0.856682 -0.584258  \n",
       "809346 -0.866583  0.073553 -1.115005 -0.668351  \n",
       "\n",
       "[303234 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding other features to POS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pca_df['score'] = df['score']\n",
    "pos_pca_df['ups'] = df['ups']\n",
    "pos_pca_df['downs'] = df['downs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_added_df = pd.concat([df, pos_pca_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text data into Tfidf Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', min_df=0.0, max_df=1.0)\n",
    "tfidf_text_vector = tfidf_vectorizer.fit_transform(df['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Latent Dirichlet Allocation to reduce dimensionality of tfidf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:796: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-1.0 * perword_bound)\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=500)\n",
    "tfidf_lda = lda.fit_transform(tfidf_text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing train-test split for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tfidf_lda, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pos, X_test_pos, Y_train_pos, Y_test_pos = train_test_split(pos_pca_df, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier with tfidf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score:  0.5008508299439376\n",
      "Test data score:  0.5009959239668114\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=30)\n",
    "rfc.fit(X_train, Y_train)\n",
    "print(\"Train data score: \", rfc.score(X_train, Y_train))\n",
    "print(\"Test data score: \", rfc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier with POS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score:  0.5785819500934374\n",
      "Test data score:  0.5741270825363743\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=30)\n",
    "rfc.fit(X_train_pos, Y_train_pos)\n",
    "print(\"Train data score: \", rfc.score(X_train_pos, Y_train_pos))\n",
    "print(\"Test data score: \", rfc.score(X_test_pos, Y_test_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier with tfidf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score:  0.5017742112784435\n",
      "Test data score:  0.500850822461713\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=0.3)\n",
    "gbc.fit(X_train, Y_train)\n",
    "print(\"Train data score: \", gbc.score(X_train, Y_train))\n",
    "print(\"Test data score: \", gbc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier with POS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score:  0.599837309002968\n",
      "Test data score:  0.5883206479441755\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=0.3)\n",
    "gbc.fit(X_train_pos, Y_train_pos)\n",
    "print(\"Train data score: \", gbc.score(X_train_pos, Y_train_pos))\n",
    "print(\"Test data score: \", gbc.score(X_test_pos, Y_test_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression with tfidf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score:  0.5004419039243707\n",
      "Test data score:  0.5009827329208933\n"
     ]
    }
   ],
   "source": [
    "lrc = LogisticRegression(penalty='l1')\n",
    "lrc.fit(X_train, Y_train)\n",
    "print(\"Train data score: \", lrc.score(X_train, Y_train))\n",
    "print(\"Test data score: \", lrc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression with POS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score:  0.5314279432780037\n",
      "Test data score:  0.5285784009814138\n"
     ]
    }
   ],
   "source": [
    "lrc = LogisticRegression(penalty='l2')\n",
    "lrc.fit(X_train_pos, Y_train_pos)\n",
    "print(\"Train data score: \", lrc.score(X_train_pos, Y_train_pos))\n",
    "print(\"Test data score: \", lrc.score(X_test_pos, Y_test_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_df = pca_added_df.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = deep_df['comment']\n",
    "text = text.apply(lambda x: x.lower() if type(x)=='str' else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145147     I'm sorry but at this point if you're being ca...\n",
      "796203              Sure I'm so worried about them right now\n",
      "44502      Just wait until your hormones kick in and you'...\n",
      "349806     Dry cigars are acceptable in a survival situat...\n",
      "832598                  she *IS* the younger one, after all.\n",
      "960035                     Yes, that is totally what I said.\n",
      "912863     Yes it was the Palestinians who crossed contin...\n",
      "700344                        OH yeah stigmata is sooo sexy!\n",
      "622606     Wow.. Americans really take care of their vete...\n",
      "288830        Saving this to repost on valentine's day 2017.\n",
      "598778     its really too bad Bryant screwed up, the way ...\n",
      "77109      If they get amnesty for running over pedestria...\n",
      "403977                                     We did it Reddit!\n",
      "740713                               Kettle Corn Master Race\n",
      "746133           You mustn't ever forget the Muller Corners!\n",
      "937517     I gave an easy one to remember not long in the...\n",
      "40664               Well except that guy who hit freddy rip.\n",
      "884069     Not easy to do what he's about to *cough*Tendu...\n",
      "412367                          Ron Swanson was never a baby\n",
      "776093     Perhaps education could be considered a natura...\n",
      "636519     That's 104 straight days, without time to eat,...\n",
      "150118     YOU SJWS DONT UNDERSTAND, IT'S NECESSARY BECAU...\n",
      "789882                          STEM Master race, represent!\n",
      "293271                                     **JohnsonRising**\n",
      "748412                                  The spice must flow.\n",
      "721921     Detonate, so if you throw it down and die, you...\n",
      "750248     I would definitely like to see them playing so...\n",
      "251091     She tried getting his patreon shut down, that'...\n",
      "943820                                         \"swag\" \"yolo\"\n",
      "312866          NPR was the absolute fucking worst for this.\n",
      "                                 ...                        \n",
      "401506     Now that the primaries are getting less white,...\n",
      "30319                        Fuck yeah love a good civil war\n",
      "828701     I don't think that might be a good idea, since...\n",
      "618246     I'm your peer, and I just reviewed your scienc...\n",
      "616442     Don't ya know, competitve match making is gonn...\n",
      "980652     Except the part where Presto doesn't use NFC, ...\n",
      "153529                                                 Knox.\n",
      "846700                                     That's Portuguese\n",
      "2684                                           Awesome photo\n",
      "1009035    Unless she's had any alcohol to drink and then...\n",
      "139210     i saw a guy jump off a bridge and found $100 t...\n",
      "277224             If only huma would've blown something....\n",
      "521540     You can get them for almost 100$ cheaper after...\n",
      "368774     If you can't hit the phones and you're interes...\n",
      "168395     Does this really need to be posted every time ...\n",
      "225599                           Niente vidi, niente sacciu.\n",
      "142836     Since there's only about 2 days a month when t...\n",
      "540478                             God is a Pakistani agent.\n",
      "281055                                         Cheezwozzers.\n",
      "361353           True fans would have made this bet in June!\n",
      "675315             No, that'd give the infil too much power.\n",
      "352786     It's because, like food, military tactics are ...\n",
      "248939        minigolf has more depth than league of legends\n",
      "634790                       hahahahahahahahah It's so funny\n",
      "679676                                          Nasa quenya.\n",
      "170087     I have a masters degree in physics, but I have...\n",
      "579249     I can see this working great for pizza vans on...\n",
      "310504                 Isn't it fun to pull useless R units?\n",
      "260580                             I need to watch more porn\n",
      "580266                               Me getting on the fp :(\n",
      "Name: comment, Length: 60647, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the words with 40000 most common words. It's basically the vocabulary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40000\n",
    "tokenizer = Tokenizer(num_words=batch_size, split='(?u)\\b\\w\\w+\\b')\n",
    "tokenizer.fit_on_texts(text.values)\n",
    "X = tokenizer.texts_to_sequences(text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0, 2752])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting pos dataframe to numpy array\n",
    "X_train_pos=X_train_pos.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = deep_df['label']\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 21:58:33.918729 27252 deprecation_wrapper.py:119] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  after removing the cwd from sys.path.\n",
      "W0723 21:58:34.040745 27252 deprecation_wrapper.py:119] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 21:58:34.065050 27252 deprecation_wrapper.py:119] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0723 21:58:34.322837 27252 deprecation_wrapper.py:119] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0723 21:58:34.328788 27252 deprecation.py:506] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embed_dim=128\n",
    "lstm_out=256\n",
    "model = Sequential()\n",
    "model.add(Embedding(batch_size, embed_dim, input_length=X.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 293, 128)          5120000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 5,514,754\n",
      "Trainable params: 5,514,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 21:58:34.488517 27252 deprecation_wrapper.py:119] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0723 21:58:34.512325 27252 deprecation_wrapper.py:119] From C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 21:58:34.621148 27252 deprecation.py:323] From C:\\Users\\vivek\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45485 samples, validate on 15162 samples\n",
      "Epoch 1/10\n",
      "45485/45485 [==============================] - 586s 13ms/step - loss: 0.6911 - acc: 0.5186 - val_loss: 0.6862 - val_acc: 0.5354\n",
      "Epoch 2/10\n",
      "45485/45485 [==============================] - 695s 15ms/step - loss: 0.6592 - acc: 0.6775 - val_loss: 0.6835 - val_acc: 0.5505\n",
      "Epoch 3/10\n",
      "45485/45485 [==============================] - 1040s 23ms/step - loss: 0.5126 - acc: 0.7389 - val_loss: 0.6927 - val_acc: 0.5365\n",
      "Epoch 4/10\n",
      "45485/45485 [==============================] - 1232s 27ms/step - loss: 0.4182 - acc: 0.7533 - val_loss: 0.7124 - val_acc: 0.5483\n",
      "Epoch 5/10\n",
      "45485/45485 [==============================] - 1318s 29ms/step - loss: 0.3796 - acc: 0.7579 - val_loss: 0.7303 - val_acc: 0.5481\n",
      "Epoch 6/10\n",
      "45485/45485 [==============================] - 1376s 30ms/step - loss: 0.3688 - acc: 0.7579 - val_loss: 0.7444 - val_acc: 0.5464\n",
      "Epoch 7/10\n",
      "45485/45485 [==============================] - 1413s 31ms/step - loss: 0.3641 - acc: 0.7583 - val_loss: 0.7523 - val_acc: 0.5468\n",
      "Epoch 8/10\n",
      "45485/45485 [==============================] - 1437s 32ms/step - loss: 0.3625 - acc: 0.7557 - val_loss: 0.7589 - val_acc: 0.5485\n",
      "Epoch 9/10\n",
      "45485/45485 [==============================] - 1579s 35ms/step - loss: 0.3612 - acc: 0.7585 - val_loss: 0.7635 - val_acc: 0.5468\n",
      "Epoch 10/10\n",
      "45485/45485 [==============================] - 1591s 35ms/step - loss: 0.3597 - acc: 0.7612 - val_loss: 0.7695 - val_acc: 0.5469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a28b90710>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=1000, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with POS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=128\n",
    "lstm_out=256\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(200, batch_input_shape=(412, X_train_pos.shape[1], )))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(Dense(50, activation='relu'))\n",
    "model2.add(Dense(1, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 303232 samples, validate on 75808 samples\n",
      "Epoch 1/5\n",
      "303232/303232 [==============================] - 3s 10us/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9792 - val_acc: 0.4995\n",
      "Epoch 2/5\n",
      "303232/303232 [==============================] - 3s 9us/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9792 - val_acc: 0.4995\n",
      "Epoch 3/5\n",
      "303232/303232 [==============================] - 3s 9us/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9792 - val_acc: 0.4995\n",
      "Epoch 4/5\n",
      "303232/303232 [==============================] - 3s 9us/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9792 - val_acc: 0.4995\n",
      "Epoch 5/5\n",
      "303232/303232 [==============================] - 3s 9us/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9792 - val_acc: 0.4995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26898221e80>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train_pos, Y_train_pos, epochs=5, batch_size=412, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with parent_comment and comment combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we only used the column comment to get our features but we should also consider parent_comment as it has the context of next sentence. So new data will contain both the sentences seperated by a space. It will go through the same process but with bigger sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = deep_df['parent_comment'] + ' ' + deep_df['comment']\n",
    "all_text = all_text.apply(lambda x: x.lower() if type(x)=='str' else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "tokenizer = Tokenizer(num_words=batch_size, split='(?u)\\b\\w\\w+\\b')\n",
    "tokenizer.fit_on_texts(all_text.values)\n",
    "X = tokenizer.texts_to_sequences(all_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "embed_dim=128\n",
    "lstm_out=256\n",
    "model = Sequential()\n",
    "model.add(Embedding(batch_size, embed_dim, input_length=X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45485 samples, validate on 15162 samples\n",
      "Epoch 1/10\n",
      "45485/45485 [==============================] - 4494s 99ms/step - loss: 0.6869 - acc: 0.5330 - val_loss: 0.6808 - val_acc: 0.5469\n",
      "Epoch 2/10\n",
      "45485/45485 [==============================] - 5595s 123ms/step - loss: 1.0866 - acc: 0.5311 - val_loss: 0.6894 - val_acc: 0.5069\n",
      "Epoch 3/10\n",
      "45485/45485 [==============================] - 5597s 123ms/step - loss: 0.6623 - acc: 0.6453 - val_loss: 0.6825 - val_acc: 0.5354\n",
      "Epoch 4/10\n",
      "45485/45485 [==============================] - 5628s 124ms/step - loss: 0.6183 - acc: 0.6620 - val_loss: 0.6982 - val_acc: 0.5382\n",
      "Epoch 5/10\n",
      "45485/45485 [==============================] - 5644s 124ms/step - loss: 0.5614 - acc: 0.6706 - val_loss: 0.7323 - val_acc: 0.5386\n",
      "Epoch 6/10\n",
      "45485/45485 [==============================] - 5639s 124ms/step - loss: 0.5249 - acc: 0.6748 - val_loss: 0.7792 - val_acc: 0.5313\n",
      "Epoch 7/10\n",
      "45485/45485 [==============================] - 5690s 125ms/step - loss: 0.5117 - acc: 0.6753 - val_loss: 0.7779 - val_acc: 0.5394\n",
      "Epoch 8/10\n",
      "45485/45485 [==============================] - 12009s 264ms/step - loss: 0.5006 - acc: 0.6787 - val_loss: 0.7995 - val_acc: 0.5262\n",
      "Epoch 9/10\n",
      "22000/45485 [=============>................] - ETA: 1:06:00 - loss: 0.4871 - acc: 0.6851"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=500, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "In conclusion, deep learning is best performer with comments data which has accuracy of around 75% in training data and 56% in testing data. We can increase accuracy by adding more vocabulary to the tokenizer but it takes more computational power to create and run such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
